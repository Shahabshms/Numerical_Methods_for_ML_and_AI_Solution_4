{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " -- 4301 -- HW4 -- Q2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNDtkXQr4asj6T8J4jbIHDA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shahabshms/Numerical_Methods_for_ML_and_AI_Solution_4/blob/main/_4301_HW4_Q2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxHP1cO-FPW-"
      },
      "source": [
        "#2 \n",
        "In Python, write a function that takes as input the matrix $A$ and an integer $k > 0$ and returns a matrix $C$ and $U$ obtained by running block coordinate descent to convergence from\n",
        "a random starting point."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLq3UXDbFvtr"
      },
      "source": [
        "As we metntioned in part 1, the objective function is\n",
        "\\begin{align*}\n",
        "f(C,U) &= \\sum_{i=1}^m\\sum_{j=1}^n \\left(A_{i,j}\\log(\\frac{A_{i,j}}{\\sum_{k=1}^K C_{i,k}U_{k,j}}) - A_{i,j} + \\sum_{k=1}^K C_{i,k}U_{k,j}\\right).\n",
        "\\end{align*}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GznY95QNiPd"
      },
      "source": [
        "As usual, for implementing gradient descent we need gradient. So, for $a\\in\\{1,\\dots,m\\}$ and $b\\in\\{1,\\dots,n\\}$ we have\n",
        "\\begin{align*}\n",
        "\\frac{\\partial f(C,U)}{\\partial C_{a,b}} &= \\sum_{j=1}^n \\left(-A_{a,j}\\frac{U_{b,j}}{\\sum_{k=1}^K C_{a,k}U_{k,j}} + U_{b,j}\\right).\n",
        "\\end{align*}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DplOMt1TFLhY"
      },
      "source": [
        "def get_gradient_wrt_C(C,U):\n",
        "  gradient_C = np.zeros(C.shape)\n",
        "  for a in range(C.shape[0]):\n",
        "    for b in range(C.shape[1]):\n",
        "      gradient_C[a,b] = get_derivative_wrt_C(C,U,a,b)\n",
        "  return gradient_C\n",
        "\n",
        "def get_derivative_wrt_C(C,U,a,b):\n",
        "  derivative = 0\n",
        "  for j in range(U.shape[1]):\n",
        "    denum = 0\n",
        "    for k in range(U.shape[0]):\n",
        "      denum += C[a,k]*U[k,j]\n",
        "    \n",
        "    if denum == 0: # ?????? \n",
        "      return 0\n",
        "      \n",
        "    derivative += -A[a,j] * (U[b,j] / denum ) + U[b,j]\n",
        "\n",
        "  return derivative"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICQMyPFO6xV1"
      },
      "source": [
        "(If you pay attention you see that there is a small trick in the implemenation. Since it is possible to have $(CU)_{a,b} = 0$, the fraction $\\frac{A_{a,b}}{(CU)_{a,b}}$ may not be a number. So, if we encounter $(CU)_{a,b} = 0$, we postpone updating that particlar entry to the next iteration. Or in other words, we put the derivative equal to zero.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgwOWw3WMzLr"
      },
      "source": [
        "And\n",
        "\\begin{align*}\n",
        "\\frac{\\partial f(C,U)}{\\partial U_{a,b}} = \\sum_{i=1}^m \\left(-A_{i,b}\\frac{C_{i,a}}{\\sum_{k=1}^K C_{i,k}U_{k,b}} + C_{i,a}\\right).\n",
        "\\end{align*}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYUWq1MPM0v-"
      },
      "source": [
        "def get_gradient_wrt_U(C,U):\n",
        "  gradient_U = np.zeros(U.shape)\n",
        "  for a in range(U.shape[0]):\n",
        "    for b in range(U.shape[1]):\n",
        "      gradient_U[a,b] = get_derivative_wrt_U(C,U,a,b)\n",
        "  return gradient_U\n",
        "\n",
        "def get_derivative_wrt_U(C,U,a,b):\n",
        "  derivative = 0\n",
        "  for i in range(C.shape[0]):\n",
        "    denum = 0\n",
        "    for k in range(C.shape[1]):\n",
        "      denum += C[i,k]*U[k,b]\n",
        "\n",
        "    if denum == 0: # ??????\n",
        "      return 0\n",
        "\n",
        "    derivative += -A[i,b] * (C[i,a] / denum ) + C[i,a]\n",
        "\n",
        "  return derivative"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYhU_ZuzukvO"
      },
      "source": [
        "def projected_gradient_descent(initial_C,initial_U,max_iterations):\n",
        "  C = initial_C\n",
        "  U = initial_U\n",
        "\n",
        "  for iteration in range(max_iterations):\n",
        "    nu = 0.01 # Learning Rate\n",
        "    \n",
        "    gradient_wrt_C = get_gradient_wrt_C(C,U)\n",
        "    C = C - nu*gradient_wrt_C # Update C\n",
        "    C[C<0] = 0 # projection\n",
        "\n",
        "    gradient_wrt_U = get_gradient_wrt_U(C,U) \n",
        "    U = U - nu*gradient_wrt_U # Update U\n",
        "    U[U<0] = 0 # projection\n",
        "\n",
        "    if np.max(abs(A-C@U)) < np.max(A)/50:\n",
        "      break\n",
        "\n",
        "  return C,U"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBnN5guH0MVL"
      },
      "source": [
        "Try to compare the impact of large $K$ and small $K$.\n",
        "Also, try different learning rates. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gVZ02uIvX_R",
        "outputId": "e581017c-559d-4b24-acea-332e22386aa0"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "m = 5\n",
        "n = 4\n",
        "K = 2\n",
        "A = np.random.randint(50,size = (m,n))\n",
        "\n",
        "##################################################################################\n",
        "# m=5  #To see that this algorithm may not converge, look at this example.\n",
        "# n=5\n",
        "# K=2\n",
        "# A = np.matrix([[ 6, 47, 49, 40, 27],\n",
        "#                [12, 48,  6, 42, 33],\n",
        "#                [27, 23, 33,  1,  4],\n",
        "#                [17, 49, 48, 27,  2],\n",
        "#                [30, 34, 27, 13, 41]])\n",
        "##################################################################################\n",
        "\n",
        "print('A: \\n',A)\n",
        "for count in range(5):\n",
        "  C = np.random.randint(10,size = (m,K))\n",
        "  U = np.random.randint(10,size = (K,n))\n",
        "\n",
        "  #print(\"initial C:\\n\",C)\n",
        "  #print(\"initial U:\\n\",U)\n",
        "\n",
        "  max_iterations = 10000\n",
        "\n",
        "  C,U = projected_gradient_descent(C,U,max_iterations)\n",
        "\n",
        "  print(\"Final B:\\n\",np.round((C@U)))\n",
        "  #print('|A-B|:\\n',np.round(abs(A - C@U)))\n",
        "  print('------')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A: \n",
            " [[ 3 27 15  6]\n",
            " [40 33  4 28]\n",
            " [47 32 25 31]\n",
            " [24 39 23  7]\n",
            " [ 6 31 42 36]]\n",
            "Final B:\n",
            " [[ 3. 18. 19. 11.]\n",
            " [47. 30.  5. 23.]\n",
            " [42. 42. 21. 29.]\n",
            " [21. 30. 21. 20.]\n",
            " [ 6. 41. 43. 25.]]\n",
            "------\n",
            "Final B:\n",
            " [[ 5. 19. 20.  6.]\n",
            " [33. 39.  4. 29.]\n",
            " [34. 51. 20. 31.]\n",
            " [39. 43.  0. 34.]\n",
            " [ 9. 10.  0.  8.]]\n",
            "------\n",
            "Final B:\n",
            " [[ 3. 18. 19. 11.]\n",
            " [47. 30.  5. 23.]\n",
            " [42. 42. 21. 29.]\n",
            " [21. 30. 21. 20.]\n",
            " [ 6. 41. 43. 25.]]\n",
            "------\n",
            "Final B:\n",
            " [[ 3. 18. 19. 11.]\n",
            " [47. 30.  5. 23.]\n",
            " [42. 42. 21. 29.]\n",
            " [21. 30. 21. 20.]\n",
            " [ 6. 41. 43. 25.]]\n",
            "------\n",
            "Final B:\n",
            " [[ 3. 18. 19. 11.]\n",
            " [47. 30.  5. 23.]\n",
            " [42. 42. 21. 29.]\n",
            " [21. 30. 21. 20.]\n",
            " [ 6. 41. 43. 25.]]\n",
            "------\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}